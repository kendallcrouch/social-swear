{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, codecs, json, os\n",
    "import ttools\n",
    "from twython import TwythonStreamer, Twython\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.parser import parse as parse_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to relate tweet with tweet_id's tweet_lang..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "BASENAMES = ['50M_250_1000_%s_updated.csv'%(NUM),'50M_1001_5000_%s_updated.csv'%(NUM)]\n",
    "BASENAMES = ['50M_250_1000_','50M_1001_5000_']\n",
    "BASEFILTERS = [(250,1001),(1001,5000)]\n",
    "for BASENUM in range(len(BASENAMES)):\n",
    "    print('working on subset files: %s'%(BASENAMES[BASENUM]))\n",
    "    \n",
    "    #load in our database of all tweets for relevant users, with new attributes we will append\n",
    "    datas = []\n",
    "    USECOLS = ['tweet_id','tweet_lang','date','text','user_id','user_name','user_screen','user_followers_count']\n",
    "    for WAVENUM in range(1,11):\n",
    "        path = '/Users/olderhorselover/USC/fall2018/csci599/teamrepo/social-swear/misc/tweets_5mAllAttributesFINAL_and_zip/tweets-5M_all_wave%s.csv'%(WAVENUM)\n",
    "        iter_data = pd.read_csv(path,usecols=USECOLS,index_col=None, lineterminator='\\n',iterator=True, chunksize=100000)\n",
    "        data = pd.concat([chunk[(chunk['user_followers_count'] >= BASEFILTERS[BASENUM][0]) & (chunk['user_followers_count'] < BASEFILTERS[BASENUM][1])].drop(labels='user_followers_count',axis=1) for chunk in iter_data])\n",
    "    #     data = data.drop(labels='user_followers_count',axis=1)\n",
    "        datas.append(data)\n",
    "        del iter_data\n",
    "        del data\n",
    "    alldatas = pd.concat(datas)\n",
    "    del datas\n",
    "    alldatas.shape\n",
    "    print(f'Raw fullset data shape: {alldatas.shape}')\n",
    "    print(f'Raw fullset data columns:\\n{alldatas.columns}')\n",
    "    print(f'Raw fullset data size: {alldatas.memory_usage().sum()/1000000:.2f} MB')\n",
    "    \n",
    "    ss = lambda x: x.replace('\\r','')\n",
    "    dd = lambda x: parse_date(x).strftime('%a %b %d %H:%M:%S %z %Y')\n",
    "    for file in os.listdir('/Users/olderhorselover/USC/fall2018/csci599/project/processedData/50m_all_updated/'):\n",
    "        if not file.startswith(BASENAMES[BASENUM]):\n",
    "            continue  #do not process file\n",
    "        print('working on file: %s'%file)\n",
    "        path = '/Users/olderhorselover/USC/fall2018/csci599/project/processedData/50m_all_updated/%s'%(file)\n",
    "        df = pd.read_csv(path,index_col=None, header=0, lineterminator='\\n',converters={'text':ss,'date':dd})\n",
    "        try:\n",
    "            df = df.drop(labels='date1\\r',axis=1)\n",
    "        except:\n",
    "            pass\n",
    "        print(f'raw processed data shape: {df.shape}')\n",
    "        print(f'raw processed data columns:\\n{df.columns}')\n",
    "        print(f'Raw processed data size: {df.memory_usage().sum()/1000000:.2f} MB')\n",
    "        print('PERFROMING THE MERGE!')\n",
    "        dfnew = df.merge(alldatas[['user_id','text','date','tweet_id','tweet_lang','user_name','user_screen']],on=['user_id','text','date'],how='left')  # new w/ new method...also drop the 'right' columns \n",
    "        print(f'orig dataframe shape: {df.shape}')\n",
    "        print(f'new dataframe shape: {dfnew.shape}')\n",
    "        print(f'perc diff shape: {dfnew.shape[0]/df.shape[0]:.3f}')\n",
    "        naStats = dfnew.isna().sum()\n",
    "        print(f'num of tweets to drop: {naStats.tweet_id}')\n",
    "        print(f'new dataframe isna() stats:\\n{naStats}')\n",
    "        try:\n",
    "            dfnew = dfnew.dropna(axis=0,how='any',subset=['tweet_id'])  #drop any tweets that didn't get a tweet_id\n",
    "        except:\n",
    "            pass\n",
    "        print(f'FINAL dataframe shape: {dfnew.shape}')\n",
    "        print('Writing dataframe to file!')\n",
    "        outname = file.replace('_updated.csv','_final')\n",
    "        alldata.to_csv('./PROCESSED_FINAL_11152018/%s.csv'%(outname),index=False)\n",
    "        alldata.to_pickle('./PROCESSED_FINAL_11152018/%s.pkl'%(outname))\n",
    "        del df\n",
    "        del dfnew\n",
    "\n",
    "    del alldatas  # no longer need/will update on next iter of BASENUM for applicable users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 15s, sys: 3.11 s, total: 3min 18s\n",
      "Wall time: 3min 19s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# #longer to read in but saves time with merge [no need to create hash, faster merge on multikey]\n",
    "# NUM = 1\n",
    "# BASENAMES = ['50M_250_1000_%s_updated.csv'%(NUM),'50M_1001_5000_%s_updated.csv'%(NUM)]\n",
    "# BASEFILTERS = [(250,1001),(1001,5000)]\n",
    "# #for BASENUM in range(len(BASENAMES))\n",
    "# ss = lambda x: x.replace('\\r','')\n",
    "# dd = lambda x: parse_date(x).strftime('%a %b %d %H:%M:%S %z %Y')\n",
    "# path = '/Users/olderhorselover/USC/fall2018/csci599/project/processedData/50m_all_updated/%s'%(BASENAMES[0])\n",
    "# df = pd.read_csv(path,index_col=None, header=0, lineterminator='\\n',converters={'text':ss,'date':dd})\n",
    "# try:\n",
    "#     df = df.drop(labels='date1\\r',axis=1)\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50M_1001_5000_4_final.csv'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.replace('_updated.csv','_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1739063, 54)\n",
      "Index(['user_id', 'text', 'n_retweets', 'n_favorites', 'n_user_followers',\n",
      "       'n_user_following', 'n_user_posts', 'n_user_favs', 'n_user_lists',\n",
      "       'user_description_text', 'user_location', 'account_age',\n",
      "       'user_created_year', 'user_created_month', 'user_geo_enabled',\n",
      "       'user_img_url', 'user_banner_url', 'tweet_truncated', 'tweet_source',\n",
      "       'tweet_coord', 'tweet_place', 'numMentions', 'date', 'engagment',\n",
      "       'is_reply', 'is_user_bot', 'is_outlier', 'is_valid', 'eggplant',\n",
      "       'mfinger', 'curse', 'fuck', 'text_polarity', 'text_subjectivity',\n",
      "       'vader_comp', 'vader_pos', 'vader_neu', 'vader_neg', 'processed_text',\n",
      "       'text_token', 'text_token_rem_stop', 'text_len', 'n_mentions',\n",
      "       'n_hashtags', 'n_links', 'n_emojis', 'swear_count', 'swear_present',\n",
      "       'swear_severity', 'swear_rarity_by_percentage', 'censored_presence',\n",
      "       'day_of_week', 'hour_of_day', 'per_engagement'],\n",
      "      dtype='object')\n",
      "Raw processed data size: 678.23 MB\n"
     ]
    }
   ],
   "source": [
    "# print(df.shape)\n",
    "# print(df.columns)\n",
    "# print(f'Raw processed data size: {df.memory_usage().sum()/1000000:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 13s, sys: 54.2 s, total: 5min 8s\n",
      "Wall time: 5min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#load in the raw dataset [has the new attributes and all tweets]...loading ALL at once may be tough on the system...\n",
    "#hmm..can we curate a new dataframe that has ALL tweets..['tweet_id','tweet_lang','date','text','user_id']\n",
    "BASENUM = 0\n",
    "datas = []\n",
    "USECOLS = ['tweet_id','tweet_lang','date','text','user_id','user_name','user_screen','user_followers_count']\n",
    "for WAVENUM in range(1,11):\n",
    "#     iter_csv = pd.read_csv('file.csv', iterator=True, chunksize=1000)\n",
    "#     data = pd.read_csv('/Users/olderhorselover/USC/fall2018/csci599/teamrepo/social-swear/misc/tweets_5mAllAttributesFINAL_and_zip/tweets-5M_all_wave%s.csv'%(WAVENUM),usecols=['tweet_id','tweet_lang','date','text','user_id'],index_col=None, header=0, lineterminator='\\n')\n",
    "    path = '/Users/olderhorselover/USC/fall2018/csci599/teamrepo/social-swear/misc/tweets_5mAllAttributesFINAL_and_zip/tweets-5M_all_wave%s.csv'%(WAVENUM)\n",
    "    iter_data = pd.read_csv(path,usecols=USECOLS,index_col=None, lineterminator='\\n',iterator=True, chunksize=100000)\n",
    "    data = pd.concat([chunk[(chunk['user_followers_count'] >= BASEFILTERS[BASENUM][0]) & (chunk['user_followers_count'] < BASEFILTERS[BASENUM][1])].drop(labels='user_followers_count',axis=1) for chunk in iter_data])\n",
    "#     data = data.drop(labels='user_followers_count',axis=1)\n",
    "    datas.append(data)\n",
    "    del iter_data\n",
    "    del data\n",
    "alldatas = pd.concat(datas)\n",
    "del datas\n",
    "alldatas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19528151, 7)\n",
      "Index(['tweet_id', 'date', 'tweet_lang', 'text', 'user_id', 'user_name',\n",
      "       'user_screen'],\n",
      "      dtype='object')\n",
      "Raw fullset data size: 1249.80 MB\n"
     ]
    }
   ],
   "source": [
    "print(alldatas.shape)\n",
    "print(alldatas.columns)\n",
    "print(f'Raw fullset data size: {alldatas.memory_usage().sum()/1000000:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE MONEY MAKER RIGHT HERE....\n",
    "\n",
    "## Need to pick the best/fastest/most reliable version..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.3 s, sys: 24.4 s, total: 1min 1s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#how='left' or 'inner' ???  ALSO...filter to 'en' tweets on our full databse beforehand? [saves memory....]\n",
    "#dfnew = df.merge(alldatas[['hasher','tweet_id','tweet_lang']],on=['hasher'])  # orig\n",
    "#dfnew = df.merge(alldatas[['hasher','tweet_id','tweet_lang','user_name','user_screen']],on=['hasher'])  # new w/ orig method\n",
    "# dfnew = df.merge(alldatas[['user_id','text','date','tweet_id','tweet_lang','user_name','user_screen']],on=['user_id','text','date'])  \n",
    "#dfnew = df.merge(alldatas[['user_id','text','date','tweet_id','tweet_lang']],on=['user_id','text','date'],how='left')  # new w/ new method...also drop the 'right' columns \n",
    "dfnew = df.merge(alldatas[['user_id','text','date','tweet_id','tweet_lang','user_name','user_screen']],on=['user_id','text','date'],how='left')  # new w/ new method...also drop the 'right' columns \n",
    "print(f'orig dataframe shape: {df.shape}')\n",
    "print(f'new dataframe shape: {dfnew.shape}')\n",
    "print(f'perc diff shape: {dfnew.shape[0]/df.shape[0]:.3f}')\n",
    "naStats = dfnew.isna().sum()\n",
    "print(f'num of tweets to drop: {naStats.tweet_id}')\n",
    "print(f'new dataframe isna() stats:\\n{naStats}')\n",
    "dfnew = dfnew.dropna(axis=0,how='any',subset=['tweet_id'])  #drop any tweets that didn't get a tweet_id\n",
    "print(dfnew.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                             0\n",
       "text                                0\n",
       "n_retweets                          0\n",
       "n_favorites                         0\n",
       "n_user_followers                    0\n",
       "n_user_following                    0\n",
       "n_user_posts                        0\n",
       "n_user_favs                         0\n",
       "n_user_lists                        0\n",
       "user_description_text          141381\n",
       "user_location                  424211\n",
       "account_age                         0\n",
       "user_created_year                   0\n",
       "user_created_month                  0\n",
       "user_geo_enabled                    0\n",
       "user_img_url                        0\n",
       "user_banner_url                 93719\n",
       "tweet_truncated                     0\n",
       "tweet_source                        0\n",
       "tweet_coord                   1725251\n",
       "tweet_place                   1626846\n",
       "numMentions                         0\n",
       "date                                0\n",
       "engagment                           0\n",
       "is_reply                            0\n",
       "is_user_bot                         0\n",
       "is_outlier                          0\n",
       "is_valid                            0\n",
       "eggplant                            0\n",
       "mfinger                             0\n",
       "curse                               0\n",
       "fuck                                0\n",
       "text_polarity                       0\n",
       "text_subjectivity                   0\n",
       "vader_comp                          0\n",
       "vader_pos                           0\n",
       "vader_neu                           0\n",
       "vader_neg                           0\n",
       "processed_text                  71184\n",
       "text_token                          0\n",
       "text_token_rem_stop                 0\n",
       "text_len                            0\n",
       "n_mentions                          0\n",
       "n_hashtags                          0\n",
       "n_links                             0\n",
       "n_emojis                            0\n",
       "swear_count                         0\n",
       "swear_present                       0\n",
       "swear_severity                      0\n",
       "swear_rarity_by_percentage          0\n",
       "censored_presence                   0\n",
       "day_of_week                         0\n",
       "hour_of_day                         0\n",
       "per_engagement                      0\n",
       "tweet_id                           61\n",
       "tweet_lang                         61\n",
       "user_name                          61\n",
       "user_screen                        61\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.000010350401337"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew.shape[0]/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new dataframe na stats:\n",
      " user_id                             0\n",
      "text                                0\n",
      "n_retweets                          0\n",
      "n_favorites                         0\n",
      "n_user_followers                    0\n",
      "n_user_following                    0\n",
      "n_user_posts                        0\n",
      "n_user_favs                         0\n",
      "n_user_lists                        0\n",
      "user_description_text          141381\n",
      "user_location                  424198\n",
      "account_age                         0\n",
      "user_created_year                   0\n",
      "user_created_month                  0\n",
      "user_geo_enabled                    0\n",
      "user_img_url                        0\n",
      "user_banner_url                 93719\n",
      "tweet_truncated                     0\n",
      "tweet_source                        0\n",
      "tweet_coord                   1725190\n",
      "tweet_place                   1626785\n",
      "numMentions                         0\n",
      "date                                0\n",
      "engagment                           0\n",
      "is_reply                            0\n",
      "is_user_bot                         0\n",
      "is_outlier                          0\n",
      "is_valid                            0\n",
      "eggplant                            0\n",
      "mfinger                             0\n",
      "curse                               0\n",
      "fuck                                0\n",
      "text_polarity                       0\n",
      "text_subjectivity                   0\n",
      "vader_comp                          0\n",
      "vader_pos                           0\n",
      "vader_neu                           0\n",
      "vader_neg                           0\n",
      "processed_text                  71166\n",
      "text_token                          0\n",
      "text_token_rem_stop                 0\n",
      "text_len                            0\n",
      "n_mentions                          0\n",
      "n_hashtags                          0\n",
      "n_links                             0\n",
      "n_emojis                            0\n",
      "swear_count                         0\n",
      "swear_present                       0\n",
      "swear_severity                      0\n",
      "swear_rarity_by_percentage          0\n",
      "censored_presence                   0\n",
      "day_of_week                         0\n",
      "hour_of_day                         0\n",
      "per_engagement                      0\n",
      "tweet_id                            0\n",
      "tweet_lang                          0\n",
      "user_name                           0\n",
      "user_screen                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.52 s, sys: 528 ms, total: 3.05 s\n",
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "%time du = dfnew.duplicated(subset=['user_id','text','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'text', 'n_retweets', 'n_favorites', 'n_user_followers',\n",
       "       'n_user_following', 'n_user_posts', 'n_user_favs', 'n_user_lists',\n",
       "       'user_description_text', 'user_location', 'account_age',\n",
       "       'user_created_year', 'user_created_month', 'user_geo_enabled',\n",
       "       'user_img_url', 'user_banner_url', 'tweet_truncated', 'tweet_source',\n",
       "       'tweet_coord', 'tweet_place', 'numMentions', 'date', 'engagment',\n",
       "       'is_reply', 'is_user_bot', 'is_outlier', 'is_valid', 'eggplant',\n",
       "       'mfinger', 'curse', 'fuck', 'text_polarity', 'text_subjectivity',\n",
       "       'vader_comp', 'vader_pos', 'vader_neu', 'vader_neg', 'processed_text',\n",
       "       'text_token', 'text_token_rem_stop', 'text_len', 'n_mentions',\n",
       "       'n_hashtags', 'n_links', 'n_emojis', 'swear_count', 'swear_present',\n",
       "       'swear_severity', 'swear_rarity_by_percentage', 'censored_presence',\n",
       "       'day_of_week', 'hour_of_day', 'per_engagement', 'tweet_id',\n",
       "       'tweet_lang', 'user_name', 'user_screen'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perc diff: 1.000\n"
     ]
    }
   ],
   "source": [
    "print(f'perc diff: {dfnew.shape[0]/df.shape[0]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50M_250_1000_\n",
      "50M_250_1000_9_updated.csv\n",
      "50M_250_1000_5_updated.csv\n",
      "50M_250_1000_6_updated.csv\n",
      "50M_250_1000_3_updated.csv\n",
      "50M_250_1000_4_updated.csv\n",
      "50M_250_1000_8_updated.csv\n",
      "50M_250_1000_1_updated.csv\n",
      "50M_250_1000_2_updated.csv\n",
      "50M_250_1000_10_updated.csv\n",
      "50M_250_1000_7_updated.csv\n",
      "50M_1001_5000_\n",
      "50M_1001_5000_3_updated.csv\n",
      "50M_1001_5000_6_updated.csv\n",
      "50M_1001_5000_5_updated.csv\n",
      "50M_1001_5000_7_updated.csv\n",
      "50M_1001_5000_2_updated.csv\n",
      "50M_1001_5000_1_updated.csv\n",
      "50M_1001_5000_8_updated.csv\n",
      "50M_1001_5000_4_updated.csv\n"
     ]
    }
   ],
   "source": [
    "BASENAMES = ['50M_250_1000_%s_updated.csv'%(NUM),'50M_1001_5000_%s_updated.csv'%(NUM)]\n",
    "BASENAMES = ['50M_250_1000_','50M_1001_5000_']\n",
    "BASEFILTERS = [(250,1001),(1001,5000)]\n",
    "for BASENUM in range(len(BASENAMES)):\n",
    "    print(BASENAMES[BASENUM])\n",
    "    for file in os.listdir('/Users/olderhorselover/USC/fall2018/csci599/project/processedData/50m_all_updated/'):\n",
    "        if not file.startswith(BASENAMES[BASENUM]):\n",
    "            continue  #do not process file\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
