{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, codecs, json, os\n",
    "import ttools\n",
    "from twython import TwythonStreamer, Twython\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image captioning! use the microsoft api...\n",
    "\n",
    "# Note: this ipynb was used for the top100. the `imageCaption_userOnly.ipynb` is for whole dataset user profile image and banner image [and no tweet images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the whole top100 dataset. It's not that big. Then, work on image captioning in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.35 s, sys: 200 ms, total: 2.54 s\n",
      "Wall time: 2.56 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = '/Users/olderhorselover/USC/fall2018/csci599/teamrepo/social-swear/misc/top100users_and_timelines_EXTENDED.csv'\n",
    "ll = lambda x: eval(x) if x else None  # the list of urls needs to actually be a list...\n",
    "ii = lambda x: x.replace('_normal.jpg','_400x400.jpg')  #the image urls give all really small photos and captioner can't handle it well. luckily there's a way to get larger images\n",
    "df = pd.read_csv(path,index_col=None, lineterminator='\\n', converters={'image_urls':ll,'user_img_url':ii})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's gather the user's profile images and banner images [should be quick, only 83 users or so]. Only requires one data instance from each user. drop_duplicates can get us that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 30)\n"
     ]
    }
   ],
   "source": [
    "#since user banner url [and user image url] is one per user, don't need al lthe user's tweets..just get one\n",
    "users_only = df.drop_duplicates(subset=['user_id'],keep='first')\n",
    "print(users_only.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the fun part! call the api wrapper to get the image captions. We will just need to store a new dataframe that will have `user_id`, `user_img_caption`, `user_banner_caption`, `user_img_caption_clean`, `user_banner_caption_clean`. Then, when using this data elsewhere, we can just load up this dataframe and do a merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olderhorselover/USC/fall2018/csci599/teamrepo/social-swear/swear-env/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 551 ms, sys: 57.7 ms, total: 609 ms\n",
      "Wall time: 2min 27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olderhorselover/USC/fall2018/csci599/teamrepo/social-swear/swear-env/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session = requests.Session()  # start a session for faster calling to api\n",
    "users_only['user_banner_caption'] = users_only.apply(lambda x: ttools.imageCaption(session,[x.user_banner_url]), axis=1)  #note the labnda wraps the x in a list. this is required for our api wrapper\n",
    "users_only['user_img_caption'] = users_only.apply(lambda x: ttools.imageCaption(session,[x.user_img_url]), axis=1)  #note the labnda wraps the x in a list. this is required for our api wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, clean up the captions for easier use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 102 ms, sys: 2.35 ms, total: 104 ms\n",
      "Wall time: 102 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olderhorselover/USC/fall2018/csci599/teamrepo/social-swear/swear-env/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/olderhorselover/USC/fall2018/csci599/teamrepo/social-swear/swear-env/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "users_only['user_banner_caption_clean'] = users_only.apply(lambda x: ttools.captionCleaner(x.user_banner_caption), axis=1)\n",
    "users_only['user_img_caption_clean'] = users_only.apply(lambda x: ttools.captionCleaner(x.user_img_caption), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it! All our users now have descriptions for their profile images and their banner images, if they have one. Let's take a look at a few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783214\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://pbs.twimg.com/profile_banners/783214/1537558537\" width=\"600\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a close up of a sign.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://pbs.twimg.com/profile_images/1013798240683266048/zRim1x6M_400x400.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a close up of a bird.\n",
      "428333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://pbs.twimg.com/profile_banners/428333/1531855520\" width=\"600\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a stop sign.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://pbs.twimg.com/profile_images/925092227667304448/fAY1HUu3_400x400.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a drawing of a person.\n",
      "176566242\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://pbs.twimg.com/profile_banners/176566242/1541098373\" width=\"600\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " water, orange, sitting.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://pbs.twimg.com/profile_images/984402258732306433/Eb6dhCya_400x400.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a man standing in front of a brick wall.\n",
      "85452649\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://pbs.twimg.com/profile_banners/85452649/1488524792\" width=\"600\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a close up of blue water.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://pbs.twimg.com/profile_images/819500159365160960/AOneM0y3_400x400.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sport, water, looking.\n",
      "268414482\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://pbs.twimg.com/profile_banners/268414482/1531506386\" width=\"600\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a close up of a logo.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://pbs.twimg.com/profile_images/1017805413956423681/zptLTIk4_400x400.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a close up of a logo.\n"
     ]
    }
   ],
   "source": [
    "locs = np.random.randint(0,size=5,high=83)\n",
    "for loc in locs:\n",
    "    print(users_only.iloc[loc]['user_id'])\n",
    "    display(Image(url=users_only['user_banner_url'].iloc[loc],width=600,height=400))\n",
    "    print(users_only.iloc[loc]['user_banner_caption_clean'])\n",
    "    display(Image(url=users_only['user_img_url'].iloc[loc],width=200,height=200))\n",
    "    print(users_only.iloc[loc]['user_img_caption_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write the dataframe with `user_id`, `user_img_caption`, `user_banner_caption`, `user_img_caption_clean`, `user_banner_caption_clean` fields. Then, when processing the full dataset [for top100] just read that frame in and merge on the `user_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.3 ms, sys: 4.85 ms, total: 10.2 ms\n",
      "Wall time: 12.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "users_only = users_only[['user_id','user_img_caption','user_banner_caption','user_img_caption_clean','user_banner_caption_clean']]\n",
    "users_only.to_csv('./ADDITIONAL_FEATURES/top100_user_img_and_banner_captions.csv',index=False)\n",
    "users_only.to_pickle('./ADDITIONAL_FEATURES/top100_user_img_and_banner_captions.pkl')\n",
    "del users_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW, to do this for each of the individual tweets... \n",
    "specifically, we use `image_urls` [`user_id` and `category` not fully necessary but helps us w/ some cool stats] and we want to create a dataframe that will have `tweet_id`, `tweet_img_caption`, `tweet_img_caption_clean`, `tweet_num_imgs`\n",
    "this will be tough timewise because look at the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number of tweets: 203993\n",
      "number of tweets with at least one image: 65150\n",
      "total number of images: 72595\n",
      "expected runtime for all images: 17.9 hours\n"
     ]
    }
   ],
   "source": [
    "print(f'original number of tweets: {df.shape[0]}')\n",
    "t = df[~df['image_urls'].isna()]  # get the tweets that have at least one image in them\n",
    "t = t[['tweet_id','image_urls','user_id','category']].drop_duplicates(subset=['tweet_id'])  # no need to keep the whole dataframe\n",
    "t['tweet_num_imgs'] = t.apply(lambda x: len(x.image_urls),axis=1)  # get the number of images per tweet with images\n",
    "print(f'number of tweets with at least one image: {t.shape[0]}')\n",
    "print(f'total number of images: {t.tweet_num_imgs.sum()}')\n",
    "runtime83 = 73.5  # seconds\n",
    "print(f'expected runtime for all images: {(((t.tweet_num_imgs.sum())/83)*runtime83)/3600:0.1f} hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick pseudo analysis shows the types of users in the top100 that post the most photos in their tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tweet_num_imgs_sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>627673190</th>\n",
       "      <th>athlete</th>\n",
       "      <td>4047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96951800</th>\n",
       "      <th>athlete</th>\n",
       "      <td>2783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19895282</th>\n",
       "      <th>artist</th>\n",
       "      <td>2779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18839785</th>\n",
       "      <th>politician</th>\n",
       "      <td>2691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31348594</th>\n",
       "      <th>artist</th>\n",
       "      <td>2598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759251</th>\n",
       "      <th>company</th>\n",
       "      <td>2473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26257166</th>\n",
       "      <th>company</th>\n",
       "      <td>2375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428333</th>\n",
       "      <th>company</th>\n",
       "      <td>2349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557521</th>\n",
       "      <th>company</th>\n",
       "      <td>2292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11348282</th>\n",
       "      <th>company</th>\n",
       "      <td>2232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      tweet_num_imgs_sum\n",
       "user_id   category                      \n",
       "627673190 athlete                   4047\n",
       "96951800  athlete                   2783\n",
       "19895282  artist                    2779\n",
       "18839785  politician                2691\n",
       "31348594  artist                    2598\n",
       "759251    company                   2473\n",
       "26257166  company                   2375\n",
       "428333    company                   2349\n",
       "2557521   company                   2292\n",
       "11348282  company                   2232"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cool! see how many images each user has posted...\n",
    "imcount = t.groupby(['user_id','category']).agg({'tweet_num_imgs': 'sum'})  #just like (t[t['user_id']==428333])['tweet_num_imgs'].sum() but does for EVERY user_id\n",
    "imcount.nlargest(n=10,columns=['tweet_num_imgs']).rename(columns={'tweet_num_imgs':'tweet_num_imgs_sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, companies occupy half of the top 10 image posters! This is surprising, seeing that they only represent ~17% of the top100 users list. It would be interesting to see how OFTEN companies tweet compared to other categories and also the ENGAGEMENT score...do the companies spend more time honing their engagement and have they identified that tweets with images get more engagement? We can test the latter [tweets w/ more images get more engagement...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist            0.674699\n",
       "company           0.168675\n",
       "athlete           0.072289\n",
       "politician        0.060241\n",
       "businessLeader    0.024096\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_only = df.drop_duplicates(subset=['user_id'],keep='first')\n",
    "users_only['category'].value_counts()/users_only.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STUDY THIS LATER\n",
    "# dfc = df.copy()\n",
    "# dfc['score'] = (dfc['retweet_count'] + dfc['favorite_count'])/dfc['user_followers_count']\n",
    "# dfc.groupby(['category']).agg({'score': 'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Okay..now that that tangent is over, let's start captioning for ALL the images\n",
    "To guard ourselves, let's process this in chucks and run as time permits. Specifically, lets keep each loop iter to ~30 mins. Based on our estimate from above, this would be about **2000 images per chunk!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on chunk 33 of 37\n",
      "No images in data chunk 33. Skipping this chunk\n",
      "Working on chunk 34 of 37\n",
      "No images in data chunk 34. Skipping this chunk\n",
      "Working on chunk 35 of 37\n",
      "No images in data chunk 35. Skipping this chunk\n",
      "Working on chunk 36 of 37\n",
      "No images in data chunk 36. Skipping this chunk\n",
      "0\n",
      "CPU times: user 24.1 ms, sys: 30.5 ms, total: 54.6 ms\n",
      "Wall time: 69.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CHUNKSIZE = 2000\n",
    "imgdfs = np.split(t,np.arange(CHUNKSIZE, t['tweet_num_imgs'].sum(), CHUNKSIZE))  # splits into list of dataframes of length=CHUNKSIZE!\n",
    "lenVerify = []\n",
    "for i,idf in enumerate(imgdfs[33:]):  #step thru each dataframe chunk!  # 14: because we interrupted our processing...\n",
    "    i = i+33  # because we interrupted our processing...\n",
    "    print('Working on chunk %s of %s'%(i,len(imgdfs)))\n",
    "    if len(idf['image_urls']) < 1:  #no images in this data chunk, skip\n",
    "        print('No images in data chunk %s. Skipping this chunk'%i)\n",
    "        continue\n",
    "    lenVerify.append(idf.shape[0])\n",
    "    startTime = time()\n",
    "    session = requests.Session()  # start a session for faster calling to api\n",
    "    print('starting the captioning')\n",
    "    idf['tweet_img_caption'] = idf.apply(lambda x: ttools.imageCaption(session,x.image_urls), axis=1)  #note the labnda no longer wraps the x in a list as it did for user images. tweet img urls already live in a list!. this is required for our api wrapper\n",
    "    print('done with captioning')\n",
    "    print('starting cleaning of captioning')\n",
    "    idf['tweet_img_caption_clean'] = idf.apply(lambda x: ttools.captionCleaner(x.tweet_img_caption), axis=1)\n",
    "    print('done with cleaning of captioning, writing to csv/pkl')\n",
    "    \n",
    "    idf.to_csv('./ADDITIONAL_FEATURES/top100_tweet_img_captions_chunk_%s.csv'%(i),index=False)\n",
    "    idf.to_pickle('./ADDITIONAL_FEATURES/top100_tweet_img_captions_chunk_%s.pkl'%(i))\n",
    "\n",
    "    endTime = time()\n",
    "    print('done writing. loop time for chunk %s'%(i))\n",
    "    print(f'{(endTime - startTime)/60:0.2f} minutes\\n\\n\\n')\n",
    "print(sum(lenVerify))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on chunk 0 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 0\n",
    "35.70 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 1 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 1\n",
    "40.44 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 2 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 2\n",
    "35.68 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 3 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 3\n",
    "43.72 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 4 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 4\n",
    "38.66 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 5 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 5\n",
    "43.12 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 6 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 6\n",
    "42.28 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 7 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 7\n",
    "44.90 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 8 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 8\n",
    "77.62 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 9 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 9\n",
    "40.82 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 10 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing.\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 11 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 11\n",
    "33.95 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 12 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 12\n",
    "45.15 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 13 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 13\n",
    "37.42 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 14 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 14\n",
    "34.59 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 15 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 15\n",
    "34.77 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 16 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 16\n",
    "40.48 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 17 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 17\n",
    "32.47 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 18 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 18\n",
    "34.93 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 19 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 19\n",
    "41.32 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 20 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 20\n",
    "34.60 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 21 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 21\n",
    "31.09 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 22 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 22\n",
    "37.68 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 23 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 23\n",
    "35.42 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 24 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 24\n",
    "38.12 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 25 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 25\n",
    "41.29 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 26 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 26\n",
    "47.01 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 27 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 27\n",
    "66.19 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 28 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 28\n",
    "31.54 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 29 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 29\n",
    "26.83 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 30 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 30\n",
    "37.45 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 31 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 31\n",
    "31.58 minutes\n",
    "\n",
    "\n",
    "\n",
    "Working on chunk 32 of 37\n",
    "starting the captioning\n",
    "done with captioning\n",
    "starting cleaning of captioning\n",
    "done with cleaning of captioning, writing to csv/pkl\n",
    "done writing. loop time for chunk 32\n",
    "20.91 minutes\n",
    "\n",
    "Working on chunk 33 of 37\n",
    "No images in data chunk 33. Skipping this chunk\n",
    "Working on chunk 34 of 37\n",
    "No images in data chunk 34. Skipping this chunk\n",
    "Working on chunk 35 of 37\n",
    "No images in data chunk 35. Skipping this chunk\n",
    "Working on chunk 36 of 37\n",
    "No images in data chunk 36. Skipping this chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
