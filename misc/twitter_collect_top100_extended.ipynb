{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTE:** this notebook copied from `twitter_collect_top100.ipynb` but modified to include 'extended' tweet information. This ensures tweet text is NOT truncated [for tweets longer than 140chars in length] AND it provides URLs for any images contained within the tweet!\n",
    "\n",
    "## NOTE: No need to run this notebook. I supplied it so you can see HOW the twitter data is collected :)\n",
    "## For actually USING the data collected here look at the <b>'twitter_unpackTop100_example.ipynb'</b> notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Modules\n",
    "- ttools has helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, codecs, json\n",
    "import ttools\n",
    "from twython import TwythonStreamer, Twython\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top100 [from pre-made json file]\n",
    "First, load the dictionary with the top100 most followed twtter users and extract the user_ids for use in api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100file = './top100_id_dictionary.json'\n",
    "top100 = ttools.json_to_dict(top100file)  # format is {user_id:[username,name]} really we just care about the user ids for now\n",
    "top100ids = [int(uid) for uid in top100.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up twitter API, get user metadata, and remove non-english accounts\n",
    "Initialize the api connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_key = \"YOUR_API_KEY_HERE\"  #api_key\n",
    "app_sec = \"YOUR_API_SECRET_KEY_HERE\"  #api_secretKey\n",
    "user_key = \"YOUR_ACCESS_TOKEN_HERE\"  #access_token\n",
    "user_sec = \"YOUR_ACCESS_TOKEN_SECRET_HERE\"  #access_token_secret\n",
    "api = ttools.initAPI(app_key,app_sec,user_key,user_sec)\n",
    "credentials = api.verify_credentials()  #KRC__verify the connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all users metadata from direct users_lookup api [can gather 100 users in a single api call..how convenient!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdata = api.lookup_user(user_id=[top100ids])\n",
    "#for some reason the api is not getting the 1st and last users....\n",
    "num1 = api.lookup_user(user_id=[top100ids[0]])\n",
    "num100 = api.lookup_user(user_id=[top100ids[-1]])\n",
    "userdata.append(num1[0])\n",
    "userdata.append(num100[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and clean the data we collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify we got all the users\n",
    "usersGotten = []\n",
    "for d in userdata:\n",
    "    usersGotten.append(int(d['id']))\n",
    "commonUsers = set(top100ids).intersection(set(usersGotten))\n",
    "if len(commonUsers) != 100:\n",
    "    print('api did not give all/correct user ids...need to investigate')\n",
    "\n",
    "#remove the non-english accounts [actually, do this in-loop below]\n",
    "# nonEnglish = []\n",
    "# for d in userdata:\n",
    "#     if d['lang'] != 'en':\n",
    "#         nonEnglish.append(d['id'])\n",
    "#         print('removing non-english account: %s'%(top100[str(d['id'])]))\n",
    "#         top100.pop(str(d['id']))\n",
    "# print('top100 is composed of %s english speakers'%(len(top100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Timeline Data and Save json\n",
    "Now, let's gather the timeline data! Note the user information we just collected is used in the 'user_info' key of the limitedUserDict [which is the one collecting ALL of the data]. The data will be saved in a *json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__of__100 total users gathered\n",
      "User ID: 27260086\n",
      "username: justinbieber\n",
      "returning from rateLimitWrapper\n",
      "1__of__100 total users gathered\n",
      "User ID: 813286\n",
      "username: BarackObama\n",
      "returning from rateLimitWrapper\n",
      "2__of__100 total users gathered\n",
      "User ID: 79293791\n",
      "username: rihanna\n",
      "returning from rateLimitWrapper\n",
      "3__of__100 total users gathered\n",
      "User ID: 17919972\n",
      "username: taylorswift13\n",
      "returning from rateLimitWrapper\n",
      "4__of__100 total users gathered\n",
      "User ID: 14230524\n",
      "username: ladygaga\n",
      "returning from rateLimitWrapper\n",
      "5__of__100 total users gathered\n",
      "User ID: 15846407\n",
      "username: TheEllenShow\n",
      "returning from rateLimitWrapper\n",
      "skipping non-english user: Cristiano\n",
      "7__of__100 total users gathered\n",
      "User ID: 10228272\n",
      "username: YouTube\n",
      "returning from rateLimitWrapper\n",
      "8__of__100 total users gathered\n",
      "User ID: 26565946\n",
      "username: jtimberlake\n",
      "returning from rateLimitWrapper\n",
      "9__of__100 total users gathered\n",
      "User ID: 25365536\n",
      "username: KimKardashian\n",
      "returning from rateLimitWrapper\n",
      "10__of__100 total users gathered\n",
      "User ID: 34507480\n",
      "username: ArianaGrande\n",
      "returning from rateLimitWrapper\n",
      "11__of__100 total users gathered\n",
      "User ID: 783214\n",
      "username: Twitter\n",
      "returning from rateLimitWrapper\n",
      "12__of__100 total users gathered\n",
      "User ID: 21111883\n",
      "username: ddlovato\n",
      "returning from rateLimitWrapper\n",
      "13__of__100 total users gathered\n",
      "User ID: 23375688\n",
      "username: selenagomez\n",
      "returning from rateLimitWrapper\n",
      "14__of__100 total users gathered\n",
      "User ID: 16409683\n",
      "username: britneyspears\n",
      "returning from rateLimitWrapper\n",
      "15__of__100 total users gathered\n",
      "User ID: 25073877\n",
      "username: realDonaldTrump\n",
      "returning from rateLimitWrapper\n",
      "16__of__100 total users gathered\n",
      "User ID: 428333\n",
      "username: cnnbrk\n",
      "returning from rateLimitWrapper\n",
      "17__of__100 total users gathered\n",
      "User ID: 44409004\n",
      "username: shakira\n",
      "returning from rateLimitWrapper\n",
      "18__of__100 total users gathered\n",
      "User ID: 15485441\n",
      "username: jimmyfallon\n",
      "returning from rateLimitWrapper\n",
      "19__of__100 total users gathered\n",
      "User ID: 50393960\n",
      "username: BillGates\n",
      "returning from rateLimitWrapper\n",
      "20__of__100 total users gathered\n",
      "User ID: 18839785\n",
      "username: narendramodi\n",
      "returning from rateLimitWrapper\n",
      "21__of__100 total users gathered\n",
      "User ID: 85603854\n",
      "username: JLo\n",
      "returning from rateLimitWrapper\n",
      "22__of__100 total users gathered\n",
      "User ID: 100220864\n",
      "username: BrunoMars\n",
      "returning from rateLimitWrapper\n",
      "23__of__100 total users gathered\n",
      "User ID: 807095\n",
      "username: nytimes\n",
      "returning from rateLimitWrapper\n",
      "24__of__100 total users gathered\n",
      "User ID: 19397785\n",
      "username: Oprah\n",
      "returning from rateLimitWrapper\n",
      "25__of__100 total users gathered\n",
      "User ID: 23083404\n",
      "username: KingJames\n",
      "returning from rateLimitWrapper\n",
      "skipping non-english user: neymarjr\n",
      "27__of__100 total users gathered\n",
      "User ID: 759251\n",
      "username: CNN\n",
      "returning from rateLimitWrapper\n",
      "28__of__100 total users gathered\n",
      "User ID: 268414482\n",
      "username: MileyCyrus\n",
      "returning from rateLimitWrapper\n",
      "29__of__100 total users gathered\n",
      "User ID: 105119490\n",
      "username: NiallOfficial\n",
      "returning from rateLimitWrapper\n",
      "30__of__100 total users gathered\n",
      "User ID: 5402612\n",
      "username: BBCBreaking\n",
      "returning from rateLimitWrapper\n",
      "31__of__100 total users gathered\n",
      "User ID: 27195114\n",
      "username: Drake\n",
      "returning from rateLimitWrapper\n",
      "skipping non-english user: instagram\n",
      "33__of__100 total users gathered\n",
      "User ID: 101311381\n",
      "username: iamsrk\n",
      "returning from rateLimitWrapper\n",
      "34__of__100 total users gathered\n",
      "User ID: 145125358\n",
      "username: SrBachchan\n",
      "returning from rateLimitWrapper\n",
      "35__of__100 total users gathered\n",
      "User ID: 26257166\n",
      "username: SportsCenter\n",
      "returning from rateLimitWrapper\n",
      "36__of__100 total users gathered\n",
      "User ID: 132385468\n",
      "username: BeingSalmanKhan\n",
      "returning from rateLimitWrapper\n",
      "37__of__100 total users gathered\n",
      "User ID: 23151437\n",
      "username: KevinHart4real\n",
      "returning from rateLimitWrapper\n",
      "38__of__100 total users gathered\n",
      "User ID: 116362700\n",
      "username: LilTunechi\n",
      "returning from rateLimitWrapper\n",
      "39__of__100 total users gathered\n",
      "User ID: 2557521\n",
      "username: espn\n",
      "returning from rateLimitWrapper\n",
      "40__of__100 total users gathered\n",
      "User ID: 20322929\n",
      "username: wizkhalifa\n",
      "returning from rateLimitWrapper\n",
      "41__of__100 total users gathered\n",
      "User ID: 84279963\n",
      "username: Louis_Tomlinson\n",
      "returning from rateLimitWrapper\n",
      "42__of__100 total users gathered\n",
      "User ID: 181561712\n",
      "username: Harry_Styles\n",
      "returning from rateLimitWrapper\n",
      "43__of__100 total users gathered\n",
      "User ID: 158314798\n",
      "username: LiamPayne\n",
      "returning from rateLimitWrapper\n",
      "44__of__100 total users gathered\n",
      "User ID: 28706024\n",
      "username: Pink\n",
      "returning from rateLimitWrapper\n",
      "45__of__100 total users gathered\n",
      "User ID: 209708391\n",
      "username: onedirection\n",
      "returning from rateLimitWrapper\n",
      "skipping non-english user: realmadrid\n",
      "47__of__100 total users gathered\n",
      "User ID: 35094637\n",
      "username: aliciakeys\n",
      "returning from rateLimitWrapper\n",
      "48__of__100 total users gathered\n",
      "User ID: 60865434\n",
      "username: KAKA\n",
      "returning from rateLimitWrapper\n",
      "49__of__100 total users gathered\n",
      "User ID: 11348282\n",
      "username: NASA\n",
      "returning from rateLimitWrapper\n",
      "50__of__100 total users gathered\n",
      "User ID: 119509520\n",
      "username: chrisbrown\n",
      "returning from rateLimitWrapper\n",
      "51__of__100 total users gathered\n",
      "User ID: 96951800\n",
      "username: FCBarcelona\n",
      "returning from rateLimitWrapper\n",
      "52__of__100 total users gathered\n",
      "User ID: 166739404\n",
      "username: EmmaWatson\n",
      "returning from rateLimitWrapper\n",
      "53__of__100 total users gathered\n",
      "User ID: 115485051\n",
      "username: ConanOBrien\n",
      "returning from rateLimitWrapper\n",
      "54__of__100 total users gathered\n",
      "User ID: 169686021\n",
      "username: kanyewest\n",
      "returning from rateLimitWrapper\n",
      "55__of__100 total users gathered\n",
      "User ID: 31348594\n",
      "username: akshaykumar\n",
      "returning from rateLimitWrapper\n",
      "56__of__100 total users gathered\n",
      "User ID: 184910040\n",
      "username: Adele\n",
      "returning from rateLimitWrapper\n",
      "57__of__100 total users gathered\n",
      "User ID: 176566242\n",
      "username: zaynmalik\n",
      "ErrorCaught in loop\n",
      "returning from rateLimitWrapper\n",
      "58__of__100 total users gathered\n",
      "User ID: 19923144\n",
      "username: NBA\n",
      "rate limit, sleeping before while loop\n",
      "ErrorCaught 1st\n",
      "59__of__100 total users gathered\n",
      "User ID: 135421739\n",
      "username: sachin_rt\n",
      "ErrorCaught 1st\n",
      "60__of__100 total users gathered\n",
      "User ID: 471741741\n",
      "username: PMOIndia\n",
      "ErrorCaught 1st\n",
      "61__of__100 total users gathered\n",
      "User ID: 90420314\n",
      "username: ActuallyNPH\n",
      "ErrorCaught 1st\n",
      "62__of__100 total users gathered\n",
      "User ID: 71201743\n",
      "username: imVkohli\n",
      "ErrorCaught 1st\n",
      "63__of__100 total users gathered\n",
      "User ID: 157140968\n",
      "username: KendallJenner\n",
      "ErrorCaught 1st\n",
      "64__of__100 total users gathered\n",
      "User ID: 31927467\n",
      "username: pitbull\n",
      "ErrorCaught 1st\n",
      "65__of__100 total users gathered\n",
      "User ID: 25521487\n",
      "username: danieltosh\n",
      "ErrorCaught 1st\n",
      "66__of__100 total users gathered\n",
      "User ID: 32959253\n",
      "username: khloekardashian\n",
      "returning from rateLimitWrapper\n",
      "67__of__100 total users gathered\n",
      "User ID: 236699098\n",
      "username: KylieJenner\n",
      "returning from rateLimitWrapper\n",
      "68__of__100 total users gathered\n",
      "User ID: 101695592\n",
      "username: deepikapadukone\n",
      "returning from rateLimitWrapper\n",
      "69__of__100 total users gathered\n",
      "User ID: 113419517\n",
      "username: iHrithik\n",
      "returning from rateLimitWrapper\n",
      "70__of__100 total users gathered\n",
      "User ID: 822215679726100480\n",
      "username: POTUS\n",
      "71__of__100 total users gathered\n",
      "User ID: 19426551\n",
      "username: NFL\n",
      "returning from rateLimitWrapper\n",
      "72__of__100 total users gathered\n",
      "User ID: 18863815\n",
      "username: coldplay\n",
      "returning from rateLimitWrapper\n",
      "73__of__100 total users gathered\n",
      "User ID: 742143\n",
      "username: BBCWorld\n",
      "returning from rateLimitWrapper\n",
      "74__of__100 total users gathered\n",
      "User ID: 88856792\n",
      "username: aamir_khan\n",
      "returning from rateLimitWrapper\n",
      "75__of__100 total users gathered\n",
      "User ID: 23617610\n",
      "username: kourtneykardash\n",
      "returning from rateLimitWrapper\n",
      "76__of__100 total users gathered\n",
      "User ID: 90836187\n",
      "username: andresiniesta8\n",
      "returning from rateLimitWrapper\n",
      "skipping non-english user: MesutOzil1088\n",
      "78__of__100 total users gathered\n",
      "User ID: 1339835893\n",
      "username: HillaryClinton\n",
      "returning from rateLimitWrapper\n",
      "79__of__100 total users gathered\n",
      "User ID: 18681139\n",
      "username: priyankachopra\n",
      "returning from rateLimitWrapper\n",
      "80__of__100 total users gathered\n",
      "User ID: 5988062\n",
      "username: TheEconomist\n",
      "returning from rateLimitWrapper\n",
      "81__of__100 total users gathered\n",
      "User ID: 44196397\n",
      "username: elonmusk\n",
      "returning from rateLimitWrapper\n",
      "82__of__100 total users gathered\n",
      "User ID: 22940219\n",
      "username: Eminem\n",
      "returning from rateLimitWrapper\n",
      "83__of__100 total users gathered\n",
      "User ID: 17471979\n",
      "username: NatGeo\n",
      "returning from rateLimitWrapper\n",
      "84__of__100 total users gathered\n",
      "User ID: 627673190\n",
      "username: ChampionsLeague\n",
      "returning from rateLimitWrapper\n",
      "85__of__100 total users gathered\n",
      "User ID: 73992972\n",
      "username: AvrilLavigne\n",
      "returning from rateLimitWrapper\n",
      "skipping non-english user: davidguetta\n",
      "skipping non-english user: MohamadAlarefe\n",
      "88__of__100 total users gathered\n",
      "User ID: 35787166\n",
      "username: NICKIMINAJ\n",
      "returning from rateLimitWrapper\n",
      "89__of__100 total users gathered\n",
      "User ID: 14920785\n",
      "username: blakeshelton\n",
      "returning from rateLimitWrapper\n",
      "90__of__100 total users gathered\n",
      "User ID: 19895282\n",
      "username: arrahman\n",
      "returning from rateLimitWrapper\n",
      "91__of__100 total users gathered\n",
      "User ID: 19248106\n",
      "username: MariahCarey\n",
      "returning from rateLimitWrapper\n",
      "92__of__100 total users gathered\n",
      "User ID: 20536157\n",
      "username: Google\n",
      "returning from rateLimitWrapper\n",
      "93__of__100 total users gathered\n",
      "User ID: 40255499\n",
      "username: ricky_martin\n",
      "returning from rateLimitWrapper\n",
      "94__of__100 total users gathered\n",
      "User ID: 379408088\n",
      "username: ShawnMendes\n",
      "returning from rateLimitWrapper\n",
      "95__of__100 total users gathered\n",
      "User ID: 1652541\n",
      "username: Reuters\n",
      "returning from rateLimitWrapper\n",
      "96__of__100 total users gathered\n",
      "User ID: 85452649\n",
      "username: edsheeran\n",
      "returning from rateLimitWrapper\n",
      "skipping non-english user: AlejandroSanz\n",
      "98__of__100 total users gathered\n",
      "User ID: 21447363\n",
      "username: katyperry\n",
      "returning from rateLimitWrapper\n",
      "99__of__100 total users gathered\n",
      "User ID: 242245578\n",
      "username: Dr_alqarnee\n",
      "returning from rateLimitWrapper\n",
      "made it to the end without error\n",
      "last user_id attempted = 242245578\n",
      "total number of users collected: 92\n",
      "finished!\n",
      "Elapsed time: 1023.1854839324951\n",
      "CPU times: user 24.4 s, sys: 2.86 s, total: 27.3 s\n",
      "Wall time: 17min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "numPasses = 1\n",
    "currentUserID = 0\n",
    "timeStart = time()\n",
    "allCollectedUsers = []  #track users we successfully got timelines for\n",
    "\n",
    "limitedUserDict = {}\n",
    "try:\n",
    "    for i,udata in enumerate(userdata):\n",
    "        user_id = int(udata['id'])\n",
    "        #skip the non-english accounts\n",
    "        if udata['lang'] != 'en':\n",
    "            print('skipping non-english user: %s'%(udata['screen_name']))\n",
    "            #top100.pop(str(d['id']))  #remove from the top100 list...not really necessary\n",
    "            continue\n",
    "        currentUserID = user_id\n",
    "        limitedUserDict[int(user_id)] = {'user_info':udata,'user_timeline':[]}  #hydrates the user info and preps the timeline list\n",
    "        #limitedUserDict[int(user_id)] = activeusers[int(user_id)]  #copy the structure for the user\n",
    "        print('%s__of__%s total users gathered'%(i,len(top100ids)))\n",
    "        print('User ID: %s'%user_id)\n",
    "        print('username: %s'%udata['screen_name'])\n",
    "        kwargs = {'user_id':int(user_id),'count':200,'exclude_replies':'false','trim_user':'true','include_rts':'false','tweet_mode':'extended'}\n",
    "        timelineTweets = ttools.rateLimitWrapperTimeline(api,api.get_user_timeline,kwargs,willingToWait=True,maxExecTime=14400)\n",
    "        limitedUserDict[user_id]['user_timeline'].extend(timelineTweets)  #extend the list\n",
    "        allCollectedUsers.append(user_id)\n",
    "        del timelineTweets\n",
    "except:\n",
    "    print('some sort of error occurred...dumping data collected so far')\n",
    "    jsonStr = json.dumps(limitedUserDict)\n",
    "    with open('top100users_and_timelines_EXTENDED.json','w') as f:\n",
    "        f.write(jsonStr)\n",
    "    del jsonStr\n",
    "    with open('top100gotten_EXTENDED.txt','w') as outF:\n",
    "        outF.write('%s'%allCollectedUsers)\n",
    "    print('last user_id attempted = %s'%currentUserID)\n",
    "    print('total number of users collected: %s'%(len(allCollectedUsers)))\n",
    "    print('finished!')\n",
    "    print('Elapsed time: %s'%(time() - timeStart))\n",
    "    sys.exit()\n",
    "#print(len(r))\n",
    "\n",
    "print('made it to the end without error')\n",
    "jsonStr = json.dumps(limitedUserDict)\n",
    "with open('top100users_and_timelines_EXTENDED.json','w') as f:\n",
    "    f.write(jsonStr)\n",
    "del jsonStr\n",
    "with open('top100gotten_EXTENDED.txt','w') as outF:\n",
    "    outF.write('%s'%allCollectedUsers)\n",
    "print('last user_id attempted = %s'%currentUserID)\n",
    "print('total number of users collected: %s'%(len(allCollectedUsers)))\n",
    "print('finished!')\n",
    "print('Elapsed time: %s'%(time() - timeStart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take the raw tweetdata and extract our defined features and put them into a dataframe. then save that dataframe as a *.csv file!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user number: 0\n",
      "user number: 1\n",
      "user number: 2\n",
      "user number: 3\n",
      "user number: 4\n",
      "user number: 5\n",
      "user number: 6\n",
      "user number: 7\n",
      "user number: 8\n",
      "user number: 9\n",
      "user number: 10\n",
      "user number: 11\n",
      "user number: 12\n",
      "user number: 13\n",
      "user number: 14\n",
      "user number: 15\n",
      "user number: 16\n",
      "user number: 17\n",
      "user number: 18\n",
      "user number: 19\n",
      "user number: 20\n",
      "user number: 21\n",
      "user number: 22\n",
      "user number: 23\n",
      "user number: 24\n",
      "user number: 25\n",
      "user number: 26\n",
      "user number: 27\n",
      "user number: 28\n",
      "user number: 29\n",
      "user number: 30\n",
      "user number: 31\n",
      "user number: 32\n",
      "user number: 33\n",
      "user number: 34\n",
      "user number: 35\n",
      "user number: 36\n",
      "user number: 37\n",
      "user number: 38\n",
      "user number: 39\n",
      "user number: 40\n",
      "user number: 41\n",
      "user number: 42\n",
      "user number: 43\n",
      "user number: 44\n",
      "user number: 45\n",
      "user number: 46\n",
      "user number: 47\n",
      "user number: 48\n",
      "user number: 49\n",
      "user number: 50\n",
      "user number: 51\n",
      "user number: 52\n",
      "user number: 53\n",
      "Handled Tweet ErrorCaught\n",
      "user number: 54\n",
      "Handled Tweet ErrorCaught\n",
      "user number: 54\n",
      "Handled Tweet ErrorCaught\n",
      "user number: 54\n",
      "Handled Tweet ErrorCaught\n",
      "user number: 54\n",
      "Handled Tweet ErrorCaught\n",
      "user number: 54\n",
      "Handled Tweet ErrorCaught\n",
      "user number: 54\n",
      "Handled Tweet ErrorCaught\n",
      "user number: 54\n",
      "Handled Tweet ErrorCaught\n",
      "user number: 54\n",
      "Handled Tweet ErrorCaught\n",
      "user number: 54\n",
      "user number: 55\n",
      "user number: 56\n",
      "user number: 57\n",
      "user number: 58\n",
      "user number: 58\n",
      "user number: 59\n",
      "user number: 60\n",
      "user number: 61\n",
      "user number: 62\n",
      "user number: 63\n",
      "user number: 64\n",
      "user number: 65\n",
      "user number: 66\n",
      "user number: 67\n",
      "user number: 68\n",
      "user number: 69\n",
      "user number: 70\n",
      "user number: 71\n",
      "user number: 72\n",
      "user number: 73\n",
      "user number: 74\n",
      "user number: 75\n",
      "user number: 76\n",
      "user number: 77\n",
      "user number: 78\n",
      "user number: 79\n",
      "user number: 80\n",
      "user number: 81\n",
      "user number: 82\n"
     ]
    }
   ],
   "source": [
    "with open('top100users_and_timelines_EXTENDED.json','r') as f:\n",
    "    readstr = f.read()\n",
    "    alldata = json.loads(readstr)\n",
    "    del readstr\n",
    "\n",
    "uNum = 0\n",
    "infos = []\n",
    "globalTweets = {}\n",
    "for uid,data in alldata.items():\n",
    "    print('user number: %s'%uNum)\n",
    "    if 'ErrorCaught' in data:\n",
    "        print('Handled User ErrorCaught')\n",
    "        continue\n",
    "    res,globalTweets = ttools.extractAllAttributes(uid,data,globalTweets,extended=True)\n",
    "    if res is None:\n",
    "        #print('extractAllAttributes returned None. Skipping')\n",
    "        continue\n",
    "    infos.append(res)\n",
    "    uNum += 1\n",
    "alldata = pd.concat(infos,axis=0,ignore_index=True)\n",
    "alldata.to_csv('./top100users_and_timelines_EXTENDED.csv',index=False)\n",
    "alldata.to_pickle('./top100users_and_timelines_EXTENDED.pkl')\n",
    "del infos\n",
    "del alldata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it! Top100 most followed users on twitter and their timelines now in file: <b>top100users_and_timelines_EXTENDED.csv</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look in 'top100users_and_timelines.csv' for example using the actual data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After the fact...add the top100 users 'category' label [artist,company,athlete,politician,businessLeader] and resave the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top100 [from pre-made json file]\n",
    "First, load the dictionary with the top100 most followed twtter users and extract the user_ids for use in api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100file = './top100_id_dictionary.json'\n",
    "top100 = ttools.json_to_dict(top100file)  # format is {user_id:[username,name]} really we just care about the user ids for now\n",
    "top100ids = [int(uid) for uid in top100.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in and inspect the top100 tweet/timeline data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.94 s, sys: 149 ms, total: 2.09 s\n",
      "Wall time: 2.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "%time top100all = pd.read_csv('top100users_and_timelines_EXTENDED.csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 203993 entries, 0 to 203992\n",
      "Data columns (total 29 columns):\n",
      "tweet_id                 203993 non-null int64\n",
      "tweet_truncated          203993 non-null bool\n",
      "date                     203993 non-null object\n",
      "tweet_source             203970 non-null object\n",
      "tweet_coord              37 non-null object\n",
      "tweet_place              364 non-null object\n",
      "text                     203993 non-null object\n",
      "text_noMentions          203951 non-null object\n",
      "is_quote_status          203993 non-null bool\n",
      "is_reply_to_status       203993 non-null bool\n",
      "is_reply_to_user         203993 non-null bool\n",
      "numMentions              203993 non-null int64\n",
      "image_urls               65150 non-null object\n",
      "retweet_count            203993 non-null int64\n",
      "favorite_count           203993 non-null int64\n",
      "user_id                  203993 non-null int64\n",
      "user_verified            203993 non-null bool\n",
      "user_description_text    162806 non-null object\n",
      "user_followers_count     203993 non-null int64\n",
      "user_friends_count       203993 non-null int64\n",
      "user_listed_count        203993 non-null int64\n",
      "user_favourites_count    203993 non-null int64\n",
      "user_statuses_count      203993 non-null int64\n",
      "user_location            131853 non-null object\n",
      "user_created_year        203993 non-null int64\n",
      "user_created_month       203993 non-null int64\n",
      "user_geo_enabled         203993 non-null bool\n",
      "user_img_url             203993 non-null object\n",
      "user_banner_url          197584 non-null object\n",
      "dtypes: bool(6), int64(12), object(11)\n",
      "memory usage: 37.0+ MB\n"
     ]
    }
   ],
   "source": [
    "top100all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to categorize the top100. Here is some helper code. The 'top100cat' dataframe is the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top100 category count:\n",
      "artist            60\n",
      "company           14\n",
      "athlete            9\n",
      "politician         7\n",
      "businessLeader     2\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##THIS WAS DONE IN twitter_unpackTop100_example.ipynb [below]\n",
    "# #turn dict into dataframe\n",
    "# top100forCategories = []\n",
    "# for uid in list(top100.keys()):\n",
    "#     top100forCategories.append([int(uid),top100[uid][0],top100[uid][1]])\n",
    "\n",
    "# #save the dataframe\n",
    "# pd.DataFrame(top100forCategories).to_csv('top100categorization.csv')\n",
    "# #then, manually labeled each entry as one of the five categories. Saved labeled file as 'top100categorization_complete.csv'\n",
    "##THIS WAS DONE IN twitter_unpackTop100_example.ipynb [above]\n",
    "\n",
    "#Now, read in the complete csv. This df can be used with the 'top100users_and_timelines.csv' dataset to help\n",
    "#categorize the top100 users/tweets into correct category\n",
    "CATEGORY = {'a':'artist','b':'businessLeader','c':'company','p':'politician','t':'athlete'}\n",
    "top100cat = pd.read_csv('top100categorization_complete.csv')\n",
    "top100cat.drop(['Unnamed: 0','notes'],axis=1,inplace=True)\n",
    "top100cat.rename(columns={'0':'user_id','1':'screenname','2':'name'},inplace=True)\n",
    "print('top100 category count:\\n%s'%(top100cat['category'].value_counts().rename(CATEGORY)))\n",
    "#top100cat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'tweet_truncated', 'date', 'tweet_source', 'tweet_coord',\n",
       "       'tweet_place', 'text', 'text_noMentions', 'is_quote_status',\n",
       "       'is_reply_to_status', 'is_reply_to_user', 'numMentions', 'image_urls',\n",
       "       'retweet_count', 'favorite_count', 'user_id', 'user_verified',\n",
       "       'user_description_text', 'user_followers_count', 'user_friends_count',\n",
       "       'user_listed_count', 'user_favourites_count', 'user_statuses_count',\n",
       "       'user_location', 'user_created_year', 'user_created_month',\n",
       "       'user_geo_enabled', 'user_img_url', 'user_banner_url', 'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decode the category labels [turn char into str]\n",
    "top100cat['category'] = top100cat['category'].apply(lambda x: CATEGORY[x])  # using dict\n",
    "\n",
    "#add the category \n",
    "res = top100all.merge(top100cat[['user_id','category']],on=['user_id'])\n",
    "res.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW, NEW FEATURES:\n",
    "**`text`**`: str:str:still tweet text, but now not truncated if >140chars`<br>\n",
    "**`image_urls`**`: list:[url1,url2,...]:list of urls for images that were found within body of the tweet!`<br>\n",
    "**`category`**`: str:str:manually labeled category type for this user`<br>\n",
    "label meanings for category = {'a':'artist','b':'businessLeader','c':'company','p':'politician','t':'athlete'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist            0.636909\n",
       "company           0.191065\n",
       "athlete           0.077660\n",
       "politician        0.067027\n",
       "businessLeader    0.027339\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check what % of top100's tweets were from what category\n",
    "res['category'].value_counts()/len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write out the *csv [and *pkl] file! good to go!\n",
    "res.to_csv('./top100users_and_timelines_EXTENDED.csv',index=False)\n",
    "res.to_pickle('./top100users_and_timelines_EXTENDED.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
